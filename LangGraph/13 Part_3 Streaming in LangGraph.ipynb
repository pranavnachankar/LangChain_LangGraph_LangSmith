{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming in LangGraph\n",
        "In LLMs, streaming means the model starts sending tokens (words) as soon as they're generated, instead of waiting for the entire response to be ready before returning it.\n",
        "\n",
        "**Why Streaming**\n",
        "1. Faster response time - low drop-off rates\n",
        "2. Mimics human like conversation(Builds trust, feels alive and keeps the user engaged)\n",
        "3. Important for Multi-modal UIs\n",
        "4. Better UX for long output such as code\n",
        "5. You can cancel midway saving tokens\n",
        "6. You can interleave Ul updates, e.g., show \"thinking...\", show tool results\n",
        "\n",
        "Doc :  https://docs.langchain.com/oss/python/langgraph/streaming"
      ],
      "metadata": {
        "id": "O1iSocB2C9zg"
      },
      "id": "O1iSocB2C9zg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the exact code as it is which was developed in last notebook `LangGraph/12 Building Chatbot with UI in LangGraph & Streamlit.ipynb`\n",
        "\n",
        "\n",
        "and for streaming we need to call `.stream` instead `.invoke`"
      ],
      "metadata": {
        "id": "2Yog2aiAvz_B"
      },
      "id": "2Yog2aiAvz_B"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "target_file = \"/content/streamlit_frontend_stream.py\"\n",
        "# ---------------------\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    print(f\"âŒ ERROR: '{target_file}' not found. Please upload it first.\")\n",
        "else:\n",
        "    # 1. Install & Setup\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"streamlit\"], check=True)\n",
        "    if not os.path.exists(\"cloudflared-linux-amd64\"):\n",
        "        subprocess.run([\"wget\", \"-q\", \"https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\"], check=True)\n",
        "        subprocess.run([\"chmod\", \"+x\", \"cloudflared-linux-amd64\"], check=True)\n",
        "\n",
        "    # 2. Cleanup old processes\n",
        "    subprocess.run([\"pkill\", \"-f\", \"streamlit\"])\n",
        "    subprocess.run([\"pkill\", \"-f\", \"cloudflared\"])\n",
        "    time.sleep(2)\n",
        "\n",
        "    # 3. Run Streamlit\n",
        "    print(f\"Starting Streamlit with {target_file}...\")\n",
        "    subprocess.Popen([\"streamlit\", \"run\", target_file, \"--server.port\", \"8501\"])\n",
        "\n",
        "    # 4. Run Tunnel\n",
        "    print(\"Starting Cloudflare Tunnel...\")\n",
        "    with open(\"cloudflared.log\", \"w\") as log_file:\n",
        "        subprocess.Popen([\"./cloudflared-linux-amd64\", \"tunnel\", \"--url\", \"http://localhost:8501\"], stdout=log_file, stderr=log_file)\n",
        "\n",
        "    # 5. Wait & Extract (Increased wait time to 10s)\n",
        "    print(\"Waiting 10 seconds for the tunnel to initialize...\")\n",
        "    time.sleep(10)\n",
        "\n",
        "    try:\n",
        "        # Fixed the regex warning by using r'' for raw string\n",
        "        cmd = r\"grep -o 'https://.*\\.trycloudflare.com' cloudflared.log | head -n 1\"\n",
        "        url_match = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip()\n",
        "\n",
        "        if url_match:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(f\"ðŸš€ Your App URL: {url_match}\")\n",
        "            print(\"=\"*40)\n",
        "        else:\n",
        "            print(\"âš ï¸ URL not found yet. Check logs manually.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "pddjmmLFvzRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1768842731003,
          "user_tz": -330,
          "elapsed": 17820,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "73f92e03-c45f-4b29-e7d0-4324464ddc8f"
      },
      "id": "pddjmmLFvzRh",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit with /content/streamlit_frontend_stream.py...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting 10 seconds for the tunnel to initialize...\n",
            "\n",
            "========================================\n",
            "ðŸš€ Your App URL: https://hands-reduced-tiger-guided.trycloudflare.com\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "langgraph_backend.py\n",
        "- -----------------\n",
        "\n",
        "```\n",
        "# Authentication\n",
        "import google.auth\n",
        "credentials, project_id = google.auth.default()\n",
        "\n",
        "# Import required libraries\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage\n",
        "from typing import TypedDict, Literal, Annotated\n",
        "\n",
        "# Initialize the Vertex AI Model with the dynamic projectID\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    project=project_id,\n",
        "    location=\"global\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "    messages : Annotated[list[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "def chat_node(state: ChatState) -> ChatState:\n",
        "    # take user query from state\n",
        "    messages = state['messages']\n",
        "\n",
        "    # pass it to LLM\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # store the received response in state\n",
        "    return{'messages':[response]}\n",
        "\n",
        "\n",
        "# check pointer\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# graph\n",
        "graph = StateGraph(ChatState)\n",
        "\n",
        "# node\n",
        "graph.add_node('chat_node',chat_node)\n",
        "\n",
        "# edges\n",
        "graph.add_edge(START,'chat_node')\n",
        "graph.add_edge('chat_node',END)\n",
        "\n",
        "# compile\n",
        "chatbot = graph.compile(checkpointer=checkpointer)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "TpoXdE-VT7KT"
      },
      "id": "TpoXdE-VT7KT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "streamlit_frontend_stream\n",
        "- ------------------------\n",
        "\n",
        "```\n",
        "import streamlit as st\n",
        "import time\n",
        "from langgraph_backend import chatbot\n",
        "from langchain_core.messages import HumanMessage, AIMessageChunk\n",
        "\n",
        "# 1. Initialize History\n",
        "if 'message_history' not in st.session_state:\n",
        "    st.session_state['message_history'] = []\n",
        "\n",
        "# 2. Display History\n",
        "for message in st.session_state['message_history']:\n",
        "    with st.chat_message(message['role']):\n",
        "        st.markdown(message['content'])\n",
        "\n",
        "# 3. Define the Generator Function (Updated for \"Fake\" Streaming)\n",
        "def get_response_generator(user_input, config):\n",
        "    \"\"\"\n",
        "    1. Gets the full response from backend (because backend uses invoke).\n",
        "    2. Breaks it into words to simulate a 'token-by-token' stream.\n",
        "    \"\"\"\n",
        "    # This call waits for the full response because backend is 'invoke'\n",
        "    response_stream = chatbot.stream(\n",
        "        {'messages': [HumanMessage(content=user_input)]},\n",
        "        config=config,\n",
        "        stream_mode=\"messages\"\n",
        "    )\n",
        "\n",
        "    for chunk, metadata in response_stream:\n",
        "        if isinstance(chunk, AIMessageChunk) or (hasattr(chunk, 'type') and chunk.type == 'ai'):\n",
        "            content = chunk.content\n",
        "            \n",
        "            # Handling Gemini 3 List Structure\n",
        "            full_text = \"\"\n",
        "            if isinstance(content, list):\n",
        "                for block in content:\n",
        "                    if isinstance(block, dict) and 'text' in block:\n",
        "                        full_text += block['text']\n",
        "            elif isinstance(content, str):\n",
        "                full_text = content\n",
        "            \n",
        "            # --- SIMULATE STREAMING ---\n",
        "            # Split by words and yield them one by one with a tiny delay\n",
        "            # This makes it look like it's typing!\n",
        "            for word in full_text.split(\" \"):\n",
        "                yield word + \" \"\n",
        "                time.sleep(0.05) # Adjust speed here (0.02 is fast, 0.1 is slow)\n",
        "\n",
        "# 4. Chat Input Logic\n",
        "user_input = st.chat_input('Type here')\n",
        "\n",
        "if user_input:\n",
        "    st.session_state['message_history'].append({'role': 'user', 'content': user_input})\n",
        "    with st.chat_message('user'):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        run_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "        \n",
        "        # st.write_stream consumes our generator\n",
        "        full_response = st.write_stream(\n",
        "            get_response_generator(user_input, run_config)\n",
        "        )\n",
        "    \n",
        "    st.session_state['message_history'].append({'role': 'assistant', 'content': full_response})\n",
        "```"
      ],
      "metadata": {
        "id": "wtHIFgiJUOPy"
      },
      "id": "wtHIFgiJUOPy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "13 Streaming in LangGraph"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
