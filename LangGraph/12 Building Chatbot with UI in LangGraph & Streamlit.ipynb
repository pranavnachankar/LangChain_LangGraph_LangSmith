{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## streamlit_frontend.py\n",
        "\n",
        "```\n",
        "import streamlit as st\n",
        "from langgraph_backend import chatbot\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Initialize session state for chat history\n",
        "if 'message_history' not in st.session_state:\n",
        "    st.session_state['message_history'] = []\n",
        "\n",
        "# Display previous messages\n",
        "for message in st.session_state['message_history']:\n",
        "    # Use the correct role for the UI (user vs assistant)\n",
        "    role = message['role']\n",
        "    with st.chat_message(role):\n",
        "        st.markdown(message['content'])\n",
        "\n",
        "# Chat Input\n",
        "user_input = st.chat_input('Type here')\n",
        "\n",
        "if user_input:\n",
        "    # 1. Add User Message to History & Display it\n",
        "    st.session_state['message_history'].append({'role': 'user', 'content': user_input})\n",
        "    with st.chat_message('user'):\n",
        "        st.markdown(user_input)\n",
        "\n",
        "    \n",
        "    # LangGraph needs a dictionary to know which \"thread\" (conversation) this is.\n",
        "    # We use a static ID \"1\" for now, but you can generate unique IDs per user.\n",
        "    run_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "    # 2. Get Response from Chatbot\n",
        "    # We pass 'run_config' instead of the Pydantic module\n",
        "    response = chatbot.invoke(\n",
        "        {'messages': [HumanMessage(content=user_input)]},\n",
        "        config=run_config\n",
        "    )\n",
        "\n",
        "    last_msg = response['messages'][-1]\n",
        "    \n",
        "    # Extract the AI's reply\n",
        "    if isinstance(last_msg.content, list):\n",
        "        # Filter for 'text' types and join them together\n",
        "        ai_message = \"\".join([\n",
        "            block['text'] for block in last_msg.content\n",
        "            if isinstance(block, dict) and block.get('type') == 'text'\n",
        "        ])\n",
        "    else:\n",
        "        # It is already a simple string\n",
        "        ai_message = last_msg.content\n",
        "\n",
        "    \n",
        "    # 3. Add AI Message to History & Display it\n",
        "    # Note: We use 'assistant' role for the AI, not 'user'\n",
        "    st.session_state['message_history'].append({'role': 'assistant', 'content': ai_message})\n",
        "    \n",
        "    with st.chat_message('assistant'):\n",
        "        st.markdown(ai_message)\n",
        "```"
      ],
      "metadata": {
        "id": "n2ic9b3F8e3J"
      },
      "id": "n2ic9b3F8e3J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## langgraph_backend.py\n",
        "\n",
        "```\n",
        "# Authentication\n",
        "import google.auth\n",
        "credentials, project_id = google.auth.default()\n",
        "\n",
        "# Import required libraries\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage\n",
        "from typing import TypedDict, Literal, Annotated\n",
        "\n",
        "# Initialize the Vertex AI Model with the dynamic projectID\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-3-flash-preview\",\n",
        "    project=project_id,\n",
        "    location=\"global\",\n",
        ")\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "    messages : Annotated[list[BaseMessage], add_messages]\n",
        "\n",
        "\n",
        "def chat_node(state: ChatState) -> ChatState:\n",
        "    # take user query from state\n",
        "    messages = state['messages']\n",
        "\n",
        "    # pass it to LLM\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # store the received response in state\n",
        "    return{'messages':[response]}\n",
        "\n",
        "\n",
        "# check pointer\n",
        "checkpointer = MemorySaver()\n",
        "\n",
        "# graph\n",
        "graph = StateGraph(ChatState)\n",
        "\n",
        "# node\n",
        "graph.add_node('chat_node',chat_node)\n",
        "\n",
        "# edges\n",
        "graph.add_edge(START,'chat_node')\n",
        "graph.add_edge('chat_node',END)\n",
        "\n",
        "# compile\n",
        "chatbot = graph.compile(checkpointer=checkpointer)\n",
        "```"
      ],
      "metadata": {
        "id": "FdDVHlOw8lR-"
      },
      "id": "FdDVHlOw8lR-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to run Streamlit over Google Colab\n",
        "- Check the line where `target_file = \"path of Streamlit application\"` is mentioned and replaced that before running it.\n",
        "- Run the code as it is\n",
        "- Once completed, it will generate `App URL`, click on it and your Streamlit application is ready to run"
      ],
      "metadata": {
        "id": "WmgMdsEt7qYU"
      },
      "id": "WmgMdsEt7qYU"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "target_file = \"/content/streamlit_frontend.py\"\n",
        "# ---------------------\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    print(f\"‚ùå ERROR: '{target_file}' not found. Please upload it first.\")\n",
        "else:\n",
        "    # 1. Install & Setup\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"streamlit\"], check=True)\n",
        "    if not os.path.exists(\"cloudflared-linux-amd64\"):\n",
        "        subprocess.run([\"wget\", \"-q\", \"https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\"], check=True)\n",
        "        subprocess.run([\"chmod\", \"+x\", \"cloudflared-linux-amd64\"], check=True)\n",
        "\n",
        "    # 2. Cleanup old processes\n",
        "    subprocess.run([\"pkill\", \"-f\", \"streamlit\"])\n",
        "    subprocess.run([\"pkill\", \"-f\", \"cloudflared\"])\n",
        "    time.sleep(2)\n",
        "\n",
        "    # 3. Run Streamlit\n",
        "    print(f\"Starting Streamlit with {target_file}...\")\n",
        "    subprocess.Popen([\"streamlit\", \"run\", target_file, \"--server.port\", \"8501\"])\n",
        "\n",
        "    # 4. Run Tunnel\n",
        "    print(\"Starting Cloudflare Tunnel...\")\n",
        "    with open(\"cloudflared.log\", \"w\") as log_file:\n",
        "        subprocess.Popen([\"./cloudflared-linux-amd64\", \"tunnel\", \"--url\", \"http://localhost:8501\"], stdout=log_file, stderr=log_file)\n",
        "\n",
        "    # 5. Wait & Extract (Increased wait time to 10s)\n",
        "    print(\"Waiting 10 seconds for the tunnel to initialize...\")\n",
        "    time.sleep(10)\n",
        "\n",
        "    try:\n",
        "        # Fixed the regex warning by using r'' for raw string\n",
        "        cmd = r\"grep -o 'https://.*\\.trycloudflare.com' cloudflared.log | head -n 1\"\n",
        "        url_match = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip()\n",
        "\n",
        "        if url_match:\n",
        "            print(\"\\n\" + \"=\"*40)\n",
        "            print(f\"üöÄ Your App URL: {url_match}\")\n",
        "            print(\"=\"*40)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è URL not found yet. Check logs manually.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOZ40Nw0hNvT",
        "outputId": "ebf71ac5-2189-4f03-cd4b-792f2c16ad1a"
      },
      "id": "jOZ40Nw0hNvT",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Streamlit with /content/streamlit_frontend.py...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting 10 seconds for the tunnel to initialize...\n",
            "\n",
            "========================================\n",
            "üöÄ Your App URL: https://dans-entrepreneur-sagem-hrs.trycloudflare.com\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZ1WcX-Mi-QO"
      },
      "id": "MZ1WcX-Mi-QO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Building a Chatbot with UI in LangGraph & Streamlit"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}